{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"arxiv_ml.csv\")\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxtTAbUh1ISK",
        "outputId": "82ae03e5-1d72-4a62-f448-8202bbf75431"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                              title  \\\n",
            "0  58805  Advances in Asynchronous Parallel and Distribu...   \n",
            "1  58806  Turbocharging Treewidth-Bounded Bayesian Netwo...   \n",
            "2  58807  Crop Yield Prediction Integrating Genotype and...   \n",
            "3  58808  Time Series Analysis and Forecasting of COVID-...   \n",
            "4  58809  Movement Tracking by Optical Flow Assisted Ine...   \n",
            "\n",
            "                                            abstract  \n",
            "0    Motivated by large-scale optimization proble...  \n",
            "1    We present a new approach for learning the s...  \n",
            "2    Accurate prediction of crop yield supported ...  \n",
            "3    Coronavirus disease 2019 (COVID-19) is a glo...  \n",
            "4    Robust and accurate six degree-of-freedom tr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Processing (Menghilangkan karakter, dsb)\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data, including 'punkt_tab'\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    #Untuk Penghilangan Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    #Untuk Karakter Spesial\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    #Untuk Melakukan Tokenisasi\n",
        "    tokens = word_tokenize (text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  #Stopword removal\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  #Lemmatization\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "#Terapkan preprocessing ke dataset\n",
        "df[\"processed_abstract\"] = df[\"abstract\"].apply(preprocess_text)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF9byeaYEIdK",
        "outputId": "7f28d492-5c1d-41f2-8a96-2f5137f2858e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                              title  \\\n",
            "0  58805  Advances in Asynchronous Parallel and Distribu...   \n",
            "1  58806  Turbocharging Treewidth-Bounded Bayesian Netwo...   \n",
            "2  58807  Crop Yield Prediction Integrating Genotype and...   \n",
            "3  58808  Time Series Analysis and Forecasting of COVID-...   \n",
            "4  58809  Movement Tracking by Optical Flow Assisted Ine...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0    Motivated by large-scale optimization proble...   \n",
            "1    We present a new approach for learning the s...   \n",
            "2    Accurate prediction of crop yield supported ...   \n",
            "3    Coronavirus disease 2019 (COVID-19) is a glo...   \n",
            "4    Robust and accurate six degree-of-freedom tr...   \n",
            "\n",
            "                                  processed_abstract  \n",
            "0  motivated large scale optimization problem ari...  \n",
            "1  present new approach learning structure treewi...  \n",
            "2  accurate prediction crop yield supported scien...  \n",
            "3  coronavirus disease 2019 covid 19 global publi...  \n",
            "4  robust accurate six degree freedom tracking po...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Membangun Sistem Retrieval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Inisialisasi model TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df[\"processed_abstract\"])\n",
        "\n",
        "def retrieve_documents(query, top_k=5):\n",
        "    query_vec = vectorizer.transform([preprocess_text(query)])  # Proses pertanyaan Alice\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()  # Hitung kemiripan\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]  # Ambil top-k artikel teratas\n",
        "    return df.iloc[top_indices][[\"title\", \"abstract\"]]\n",
        "\n",
        "# Contoh pertanyaan\n",
        "query = \"What is the latest development in deep learning?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "print(retrieved_docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iN-zsqpEbE1",
        "outputId": "766bc933-cf04-4527-adee-0d2c2695dc27"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   title  \\\n",
            "15586  Applications of Federated Learning in Smart Ci...   \n",
            "23461  There is no data like more data -- current sta...   \n",
            "14006  Label Augmentation via Time-based Knowledge Di...   \n",
            "23562  A Comprehensive Survey on Community Detection ...   \n",
            "18417        Model Complexity of Deep Learning: A Survey   \n",
            "\n",
            "                                                abstract  \n",
            "15586    Federated learning plays an important role i...  \n",
            "23461    Annotated datasets have become one of the mo...  \n",
            "14006    Detecting anomalies has become increasingly ...  \n",
            "23562    A community reveals the features and connect...  \n",
            "18417    Model complexity is a fundamental problem in...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Membangun Model Generasi Jawaban\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load model dan tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    input_text = f\"question: {question}  context: {context}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(input_ids)\n",
        "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Contoh penggunaan\n",
        "question = \"What is deep learning?\"\n",
        "context = retrieved_docs.iloc[0][\"abstract\"]  # Ambil abstrak artikel pertama\n",
        "answer = generate_answer(question, context)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyLPk-_XEkgR",
        "outputId": "a4943e52-d249-4e79-f8ca-c0f040a0c3ed"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Internet of Things, transportation, communications, finance, medical and other fields\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluasi Model (Response Relevancy)\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Simulasi y_true (ground truth) dan y_pred (hasil sistem)\n",
        "y_true = [1, 1, 0, 1, 0]  # Artikel relevan atau tidak (1=relevan, 0=tidak)\n",
        "y_pred = [1, 1, 0, 0, 1]\n",
        "\n",
        "recall_at_k = recall_score(y_true, y_pred, average='macro')\n",
        "print(f'Recall@K: {recall_at_k}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7DjPs4AEzvd",
        "outputId": "91627758-5c5e-4e0e-9838-af7412ef8cc8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@K: 0.5833333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluasi Model (Context Precision)\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer #Import rouge_scorer\n",
        "\n",
        "#Membangun Model Generasi Jawaban # Copy from ipython-input-13-9d21328a15ea and paste here\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import pandas as pd #Import pandas for reading the data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #Import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity #Import cosine_similarity\n",
        "# Assuming 'preprocess_text' function is defined in a previous cell or imported\n",
        "#from your_preprocessing_module import preprocess_text\n",
        "\n",
        "# Instead of trying to import 'preprocess_text',\n",
        "# simply copy and paste the function definition here\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data, including 'punkt_tab'\n",
        "# (This might be redundant if already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    #Untuk Penghilangan Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    #Untuk Karakter Spesial\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    #Untuk Melakukan Tokenisasi\n",
        "    tokens = word_tokenize (text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  #Stopword removal\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  #Lemmatization\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Define retrieve_documents function\n",
        "def retrieve_documents(query, top_k=5):\n",
        "    query_vec = vectorizer.transform([preprocess_text(query)])  # Proses pertanyaan Alice\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()  # Hitung kemiripan\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]  # Ambil top-k artikel teratas\n",
        "    return df.iloc[top_indices][[\"title\", \"abstract\"]]\n",
        "\n",
        "# Assuming 'df' and 'preprocess_text' are defined in previous cells\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"arxiv_ml.csv\")\n",
        "\n",
        "# Apply preprocessing to create the 'processed_abstract' column\n",
        "df[\"processed_abstract\"] = df[\"abstract\"].apply(preprocess_text)\n",
        "\n",
        "# Initialize TF-IDF model\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df[\"processed_abstract\"])\n",
        "\n",
        "# Load model dan tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    input_text = f\"question: {question}  context: {context}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(input_ids)\n",
        "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Contoh penggunaan\n",
        "query = \"What is the latest development in deep learning?\" #Define query before using it\n",
        "retrieved_docs = retrieve_documents(query) #Call retrieve_documents to define retrieved_docs\n",
        "print(retrieved_docs)\n",
        "\n",
        "question = \"What is deep learning?\"\n",
        "context = retrieved_docs.iloc[0][\"abstract\"]  # Ambil abstrak artikel pertama\n",
        "answer = generate_answer(question, context) #This will define answer variable in current scope\n",
        "print(answer)\n",
        "#End of copy\n",
        "\n",
        "reference = [\"Deep learning is a subset of machine learning.\"]\n",
        "candidate = [answer]  # Jawaban dari model\n",
        "\n",
        "# Hitung BLEU\n",
        "bleu_score = sentence_bleu(reference, candidate)\n",
        "print(f'BLEU Score: {bleu_score}')\n",
        "\n",
        "# Hitung ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(reference[0], candidate[0])\n",
        "print(f'ROUGE Scores: {scores}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0v_FJU7E210",
        "outputId": "97d2ba37-fa3b-4cba-fdc2-0d2830bd8eb8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   title  \\\n",
            "15586  Applications of Federated Learning in Smart Ci...   \n",
            "23461  There is no data like more data -- current sta...   \n",
            "14006  Label Augmentation via Time-based Knowledge Di...   \n",
            "23562  A Comprehensive Survey on Community Detection ...   \n",
            "18417        Model Complexity of Deep Learning: A Survey   \n",
            "\n",
            "                                                abstract  \n",
            "15586    Federated learning plays an important role i...  \n",
            "23461    Annotated datasets have become one of the mo...  \n",
            "14006    Detecting anomalies has become increasingly ...  \n",
            "23562    A community reveals the features and connect...  \n",
            "18417    Model complexity is a fundamental problem in...  \n",
            "Internet of Things, transportation, communications, finance, medical and other fields\n",
            "BLEU Score: 0\n",
            "ROUGE Scores: {'rouge1': Score(precision=0.1, recall=0.125, fmeasure=0.11111111111111112), 'rougeL': Score(precision=0.1, recall=0.125, fmeasure=0.11111111111111112)}\n"
          ]
        }
      ]
    }
  ]
}