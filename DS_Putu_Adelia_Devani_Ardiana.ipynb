{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqTEG3UBBqcAqP3qFYUZlf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freyaarby/Freepass-DS-BCC-2025/blob/main/DS_Putu_Adelia_Devani_Ardiana.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"arxiv_ml.csv\")\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "nxtTAbUh1ISK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Melakukan Preprocessing\n",
        "def preprocess_text(text):\n",
        "\n",
        "    #Penghilangan Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    #Penghilangan Karakter Spesial\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    #Tokenisasi\n",
        "    tokens = word_tokenize (text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "#Menerapkan preprocessing ke dataset\n",
        "df[\"processed_abstract\"] = df[\"abstract\"].apply(preprocess_text)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "tF9byeaYEIdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Membangun Sistem Retrieval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#Melakukan Inisialisasi Model TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df[\"processed_abstract\"])\n",
        "\n",
        "#Melakukan Retrieval\n",
        "def retrieve_documents(query, top_k=5):\n",
        "    query_vec = vectorizer.transform([preprocess_text(query)])\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    return df.iloc[top_indices][[\"title\", \"abstract\"]]\n",
        "\n",
        "#Analisis Contoh pertanyaan\n",
        "query = \"What is the latest development in deep learning?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "print(retrieved_docs)\n",
        "\n",
        "query1 = \"What is deep learning?\"\n",
        "retrieved_docs1 = retrieve_documents(query1)\n",
        "print(retrieved_docs1)\n",
        "\n",
        "query2 = \"What are the advantages of Transformer models compared to RNNs and CNNs in Machine Learning?\"\n",
        "retrieved_docs2 = retrieve_documents(query2)\n",
        "print(retrieved_docs2)\n",
        "\n",
        "query3 = \"How does the Attention Mechanism work in modern NLP models?\"\n",
        "retrieved_docs3 = retrieve_documents(query3)\n",
        "print(retrieved_docs3)\n",
        "\n",
        "query4 = \"What are the key differences between supervised, unsupervised, and reinforcement learning?\"\n",
        "retrieved_docs4 = retrieve_documents(query4)\n",
        "print(retrieved_docs4)\n",
        "\n",
        "query5 = \"How is fine-tuning applied to pretrained models like BERT or GPT?\"\n",
        "retrieved_docs5 = retrieve_documents(query5)\n",
        "print(retrieved_docs5)\n"
      ],
      "metadata": {
        "id": "6iN-zsqpEbE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Membangun Sistem Generasi Jawaban dengan T5\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "#Load model dan tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "#Melakukan Generasi\n",
        "def generate_answer(question, context):\n",
        "    input_text = f\"question: {question}  context: {context}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(input_ids)\n",
        "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "#Analisis Contoh Penggunaan\n",
        "question1 = \"What is deep learning?\"\n",
        "context1 = retrieved_docs.iloc[0][\"abstract\"]\n",
        "answer1 = generate_answer(question1, context1)\n",
        "print(answer1)\n",
        "\n",
        "question2 = \"What are the advantages of Transformer models compared to RNNs and CNNs in Machine Learning?\"\n",
        "context2 = retrieved_docs.iloc[0][\"abstract\"]\n",
        "answer2 = generate_answer(question2, context2)\n",
        "print(answer2)\n",
        "\n",
        "question3 = \"How does the Attention Mechanism work in modern NLP models?\"\n",
        "context3 = retrieved_docs.iloc[0][\"abstract\"]\n",
        "answer3 = generate_answer(question3, context3)\n",
        "print(answer3)\n",
        "\n",
        "question4 = \"What are the key differences between supervised, unsupervised, and reinforcement learning?\"\n",
        "context4 = retrieved_docs.iloc[0][\"abstract\"]\n",
        "answer4 = generate_answer(question4, context4)\n",
        "print(answer4)\n",
        "\n",
        "question5 = \"How is fine-tuning applied to pretrained models like BERT or GPT?\"\n",
        "context5 = retrieved_docs.iloc[0][\"abstract\"]\n",
        "answer5 = generate_answer(question5, context5)\n",
        "print(answer5)"
      ],
      "metadata": {
        "id": "pyLPk-_XEkgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluasi Model Generasi (Response Relevancy)\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "#Simulasi y_true (ground truth) dan y_pred (hasil sistem)\n",
        "y_true = [1, 1, 0, 1, 0]  #Artikel relevan atau tidak (1=relevan, 0=tidak)\n",
        "y_pred = [1, 1, 0, 0, 1]\n",
        "\n",
        "recall_at_k = recall_score(y_true, y_pred, average='macro')\n",
        "print(f'Recall@K: {recall_at_k}')\n"
      ],
      "metadata": {
        "id": "q7DjPs4AEzvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluasi Model Retrieval (Context Precision)\n",
        "#Menghitung BLEU\n",
        "bleu_score = sentence_bleu(reference, candidate)\n",
        "print(f'BLEU Score: {bleu_score}')\n",
        "\n",
        "#Menghitung ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(reference[0], candidate[0])\n",
        "print(f'ROUGE Scores: {scores}')"
      ],
      "metadata": {
        "id": "L0v_FJU7E210"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}