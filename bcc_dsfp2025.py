# -*- coding: utf-8 -*-
"""BCC_DSFP2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nl9qJem0XtsQ_aTXx0kdogPTfDWYK_aa
"""

import pandas as pd

df = pd.read_csv("arxiv_ml.csv")

print(df.head())

#Data Processing (Menghilangkan karakter, dsb)
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK data, including 'punkt_tab'
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    #Untuk Penghilangan Lowercasing
    text = text.lower()

    #Untuk Karakter Spesial
    text = re.sub(r'\W', ' ', text)

    #Untuk Melakukan Tokenisasi
    tokens = word_tokenize (text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]  #Stopword removal
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  #Lemmatization
    return " ".join(tokens)

#Terapkan preprocessing ke dataset
df["processed_abstract"] = df["abstract"].apply(preprocess_text)

print(df.head())

#Membangun Sistem Retrieval
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Inisialisasi model TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df["processed_abstract"])

def retrieve_documents(query, top_k=5):
    query_vec = vectorizer.transform([preprocess_text(query)])  # Proses pertanyaan Alice
    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()  # Hitung kemiripan
    top_indices = similarities.argsort()[-top_k:][::-1]  # Ambil top-k artikel teratas
    return df.iloc[top_indices][["title", "abstract"]]

# Contoh pertanyaan
query = "What is the latest development in deep learning?"
retrieved_docs = retrieve_documents(query)
print(retrieved_docs)

#Membangun Model Generasi Jawaban
from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load model dan tokenizer
model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

def generate_answer(question, context):
    input_text = f"question: {question}  context: {context}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    output_ids = model.generate(input_ids)
    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return answer

# Contoh penggunaan
question = "What is deep learning?"
context = retrieved_docs.iloc[0]["abstract"]  # Ambil abstrak artikel pertama
answer = generate_answer(question, context)
print(answer)

#Evaluasi Model (Response Relevancy)
from sklearn.metrics import recall_score

# Simulasi y_true (ground truth) dan y_pred (hasil sistem)
y_true = [1, 1, 0, 1, 0]  # Artikel relevan atau tidak (1=relevan, 0=tidak)
y_pred = [1, 1, 0, 0, 1]

recall_at_k = recall_score(y_true, y_pred, average='macro')
print(f'Recall@K: {recall_at_k}')

#Evaluasi Model (Context Precision)
from nltk.translate.bleu_score import sentence_bleu
!pip install rouge-score
from rouge_score import rouge_scorer #Import rouge_scorer

#Membangun Model Generasi Jawaban # Copy from ipython-input-13-9d21328a15ea and paste here
from transformers import T5ForConditionalGeneration, T5Tokenizer
import pandas as pd #Import pandas for reading the data
from sklearn.feature_extraction.text import TfidfVectorizer #Import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity #Import cosine_similarity
# Assuming 'preprocess_text' function is defined in a previous cell or imported
#from your_preprocessing_module import preprocess_text

# Instead of trying to import 'preprocess_text',
# simply copy and paste the function definition here
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK data, including 'punkt_tab'
# (This might be redundant if already downloaded)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    #Untuk Penghilangan Lowercasing
    text = text.lower()

    #Untuk Karakter Spesial
    text = re.sub(r'\W', ' ', text)

    #Untuk Melakukan Tokenisasi
    tokens = word_tokenize (text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]  #Stopword removal
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  #Lemmatization
    return " ".join(tokens)

# Define retrieve_documents function
def retrieve_documents(query, top_k=5):
    query_vec = vectorizer.transform([preprocess_text(query)])  # Proses pertanyaan Alice
    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()  # Hitung kemiripan
    top_indices = similarities.argsort()[-top_k:][::-1]  # Ambil top-k artikel teratas
    return df.iloc[top_indices][["title", "abstract"]]

# Assuming 'df' and 'preprocess_text' are defined in previous cells
# Load the dataset
df = pd.read_csv("arxiv_ml.csv")

# Apply preprocessing to create the 'processed_abstract' column
df["processed_abstract"] = df["abstract"].apply(preprocess_text)

# Initialize TF-IDF model
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df["processed_abstract"])

# Load model dan tokenizer
model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

def generate_answer(question, context):
    input_text = f"question: {question}  context: {context}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    output_ids = model.generate(input_ids)
    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return answer

# Contoh penggunaan
query = "What is the latest development in deep learning?" #Define query before using it
retrieved_docs = retrieve_documents(query) #Call retrieve_documents to define retrieved_docs
print(retrieved_docs)

question = "What is deep learning?"
context = retrieved_docs.iloc[0]["abstract"]  # Ambil abstrak artikel pertama
answer = generate_answer(question, context) #This will define answer variable in current scope
print(answer)
#End of copy

reference = ["Deep learning is a subset of machine learning."]
candidate = [answer]  # Jawaban dari model

# Hitung BLEU
bleu_score = sentence_bleu(reference, candidate)
print(f'BLEU Score: {bleu_score}')

# Hitung ROUGE
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
scores = scorer.score(reference[0], candidate[0])
print(f'ROUGE Scores: {scores}')